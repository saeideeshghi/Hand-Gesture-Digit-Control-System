# ğŸ– Hand Gesture Digit Control System ğŸš€

A real-time computer-vision system that recognizes hand-sign digits (1â€“10) using MediaPipe Hands and a lightweight SVM model, then maps each gesture to a Windows system action such as volume control, switching desktops, or opening Chrome.

The project combines deep feature extraction (via MediaPipe CNN) with classical machine learning (SVM) for fast, CPU-based gesture control.

![Processed Image](docs/1.png)

![Processed Image](docs/2.png)


# âœ¨ Key Features

Real-time Gesture Recognition: Tracks hand landmarks from webcam using MediaPipe.

Dual-Hand Support: Right hand â†’ digits 1â€“5, Left hand â†’ digits 6â€“10.

Smart Stability: EMA filtering, confidence hysteresis, dwell confirmation, and cooldown logic prevent false triggers.

Action Mapping: Each gesture triggers a unique Windows function (volume, explorer, chrome, etc.).

Interactive HUD: On-screen progress bar and target badge showing detected gesture, confidence, and action name.

Safety Controls: On-screen toggles (Safety, Confirm, ShowOnly, HandRule, Execute).

# ğŸ§  System Architecture
```txt
[Webcam RGB Frame]
        â†“
[MediaPipe Hands (Deep CNN) â†’ 21 Landmarks]
        â†“
[Feature Engineering (102D vector)]
        â†“
[StandardScaler + SVM (RBF)]
        â†“
[Gesture Class (1â€“10)]
        â†“
[Action Mapping Layer (Windows API + pyautogui)]
```
Deep Learning Used:
MediaPipe Hands internally uses a CNN to extract hand landmarks.
The SVM model is trained on those deep features for gesture classification.

# ğŸ“Š Gesture â†’ Action Mapping
| Digit |  Hand | Action             |
| :---: | :---: | :----------------- |
|   1   | Right | Volume Up          |
|   2   | Right | Volume Down        |
|   3   | Right | Switch Desktop     |
|   4   | Right | Snipping Tool      |
|   5   | Right | Open File Explorer |
|   6   |  Left | Open Chrome        |
|   7   |  Left | Mute / Unmute      |
|   8   |  Left | Lock Screen        |
|   9   |  Left | Minimize All       |
|   10  |  Left | Show Desktop       |

# ğŸ›  Installation & Setup
Prerequisites

-  Python 3.8 or higher

-  Windows OS (for system actions)

-  Webcam

## Required Libraries

Create requirements.txt:
```txt
Prerequisites

Python 3.8 or higher

Windows OS (for system actions)

Webcam

Required Libraries

Create requirements.txt:
```
Install dependencies:
```txt
pip install -r requirements.txt
```
# â–¶ï¸ Running the Application
Place the trained model files in your project folder:
```txt
model_digits_svm.pkl
scaler_digits.pkl
sign_digits_user.npz
```
Run the main notebook or Python file:
```txt
Run the main notebook or Python file:
```
Use your webcam to show hand digits 1â€“10.

Hold a gesture steady until the progress bar fills.

Press Enter or click EXECUTE to trigger the mapped system action.

# ğŸ“‚ Project Structure
```txt
.
â”œâ”€â”€ gesture_control.py          # Main application script
â”œâ”€â”€ sign_digits_user.npz        # Collected training dataset
â”œâ”€â”€ model_digits_svm.pkl        # Trained SVM model
â”œâ”€â”€ scaler_digits.pkl           # StandardScaler object
â”œâ”€â”€ Untitled.ipynb              # Jupyter notebook (data collection & training)
â”œâ”€â”€ requirements.txt            # Dependencies list
â””â”€â”€ README.md                   # Project documentation
```
# ğŸ” Technical Details
Feature Vector: 102D
(42 XY coordinates + 55 pairwise distances + 5 finger angles)

Model: SVM with RBF kernel (C=5.0, gamma='scale', probability=True)

Stabilization:

EMA_ALPHA = 0.45

TEMP = 0.7

CONF_ENTER = 0.70, CONF_EXIT = 0.55

DWELL_MS = 650, COOLDOWN_S = 0.8

# ğŸ§© Future Improvements
Replace SVM with MLP (deep model) for end-to-end learning.

Add gesture sequences (e.g., swipe, rotate).

Add GNN for hand-skeleton modeling.

Support for Linux/macOS actions.

JSON config for customizable mappings.

# ğŸ“œ License

This project is licensed under the Apache License.
